---
title: "Aspden_lab_from_paper"
author: "Isabel Birds"
date: "09/12/2020"
output: html_document
---

```{r setup}
library(rJava)
library(tabulizer)
library(knitr)
library(tidyverse)
library(stm)
library(magrittr)
library(tidytext)
library(overviewR)
library(readtext)
library(quanteda)
library(wesanderson)

theme_set(
  theme_minimal() + theme(
    strip.background = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank()
  )
)
```

https://nlp-bergen.netlify.app/#Knowing_terms_and_concepts 

Scrape text from PDF

To avoid headers on preprints or similar, use locate_areas to work out area to include.
Only include pages w/o references etc - unless you want them!

```{r text}
#Extract to end of papers - no refs
aspden <- extract_text("../papers/aspden.pdf", pages = c(1:16))
lncRNA_review_s <- extract_text("../papers/lncRNA_review.pdf", page = 1, area = list(c(243.390,36.958,790.020,558.318 )))
lncRNA_review <- extract_text("../papers/lncRNA_review.pdf", page = c(2:12))
neuro <- extract_text("../papers/neuro_lncRNA.pdf", page = c(1:20),area=list(c(74,67,769,548)))

#combine papers
text <- c(aspden,lncRNA_review,neuro)

#removed all refs in brackets
text <- gsub("\\s*\\([^\\)]+\\)","",text)

#combine all words split over lines
text <- gsub("-\n","",text)

#preprocess and clean
token <- tokens(text, 
                remove_numbers = TRUE, 
                remove_punct = TRUE, 
                remove_symbols =  TRUE,
                remove_url = TRUE, 
                split_hyphens = TRUE)

token_ungd <- tokens_select(
  token,
  c("[\\d-]","[[:punct:]]", "^.{1,2}$"),
  selection = "remove",
  valuetype = "regex",
  verbose = TRUE
)

#Calculate a document-feature matrix (DFM)
mydfm <- dfm(
  # Take the token object
  token_ungd,
  # Lower the words
  tolower = TRUE,
  # Remove stop words
  remove = stopwords("english")
)

head(mydfm)

#Trim data: remove all the words that appear less than 5% of the time 
mydfm.trim <-
  dfm_trim(
    mydfm,
    min_docfreq = 0.05,
    # min 7.5%
    #max_docfreq = 0.90,
    #  max 90%
    docfreq_type = "prop"
  ) 

head(mydfm.trim)
```


Make dict 
```{r dict}

#prep dict
dict <- colnames(mydfm.trim)

#open eng dict for filter
eng_dict <- read.delim("../dict_store/wordsEnGb.txt", header=F, sep=c("\n"," ","\t",""))

filter_dict <- dict[!dict %in% eng_dict$V1]
filter_dict

#open aspden dict to add
asp_names <- read.delim("../dict_store/Aspden_names.txt",header=F, sep=c("\n"," ","\t",""))

filter_dict <- c(filter_dict,asp_names$V1) %>% sort() %>% unique()

write(filter_dict,file="../final_dicts/VSCode/Aspden_dict.txt")
write(filter_dict,file="../final_dicts/MSWord/Aspden_dict.dic")
```

Visualise

```{r plots}

quanteda::textplot_wordcloud(
  # Load the DFM object
  mydfm,
  # Define the minimum number the words have to occur
  min_count = 3,
  # Define the maximum number the words can occur
  max_words = 500,
  # Define a color
  color = wes_palette("Darjeeling1")
)

# Get the 30 top features from the DFM
freq_feature <- topfeatures(mydfm, 30)

# Create a data.frame for ggplot
data <- data.frame(list(
  term = names(freq_feature),
  frequency = unname(freq_feature)
))

# Plot the plot
data %>%
  # Call ggplot
  ggplot() +
  # Add geom_segment (this will give us the lines of the lollipops)
  geom_segment(aes(
    x = reorder(term, frequency),
    xend = reorder(term, frequency),
    y = 0,
    yend = frequency
  ), color = "grey") +
  # Call a point plot with the terms on the x-axis and the frequency on the y-axis
  geom_point(aes(x = reorder(term, frequency), y = frequency)) +
  # Flip the plot
  coord_flip() +
  # Add labels for the axes
  xlab("") +
  ylab("Absolute frequency of the features")
```